Run/Stop:
> docker compose -f kafka-cluster.yml up -d

Login & exec Kafka cli:
> docker exec -it kafka-1 /bin/bash
> cd /opt/bitnami/kafka/bin

Create topics (using kafdrop on localhost:9000 - Topic ACL / New ):
    1. 3 Partitions 2 Replicas
        ./kafka-topics.sh \
            --bootstrap-server localhost:29092 \
            --create \
            --topic kafka.learning.orders \
            --partitions 3 \
            --replication-factor 2
    2. 4 Partitions 3 Replicas
        ./kafka-topics.sh \
            --bootstrap-server localhost:29092 \
            --create \
            --topic kafka.learning.tweets \
            --partitions 4 \
            --replication-factor 3

- start consumer 1st, then producer
- Check current controller status:
    > log in into stand alone broker (kafka-3)
        > docker exec -it kafka-3 /bin/bash
        > cd /opt/bitnami/kafka/bin
        > ./kafka-metadata-quorum.sh \
            --bootstrap-server localhost:19094 \
            describe --status

            ... see status, shutdown one of controllers (kafka-1 and/or kafka-2) - check the status change

############### Producer client scalability options ###############
--- publishing messages modes ---
1 Synchronous - publish/wait in the same thread/returns acknowledge or fail - Network IO bound ops slow down performance
2 Asynchronous with No-Check - fire & forget - does not wait for acknowledge
                 - local producer stores messages in a cache
                  and then publishes to Kafka in separate thread
                  client is not blocked, low latency
                  - cons - possible messages lost when network fail
3 Asynchronous with Callback - cliant code sends message to local producer, gets a callback function to process results,
                  in this part client code moves on (not blocked),
                  producer cache data and then publishes cached messages in separate thread
                  Low latency / Complex error handling
---
Acknowledgement - 0 (No acks - high throuput, no guarantees),
                  1 - default (ack from leader in replicas) (low throughput in sync mode),
                  All - all in-sync replicas should receive message (low throughput in sync mode)
---
Other options:
    BUFFER.MEMORY       (Allowed memory for buffering records)
    COMPRESSION.TYPE    (none, gzip, snappy, lz4, zstd - data compression format for lower payload size)
    BATCH.SIZE          (Size in bytes to be sent to brocker in batches)
    LINGER.MS           (time, ms, to wait more messages before sind to broker)
    
############### Consumer client scalability options ###############
--- Gropus ---
- Group Coordinator (Kafka broker in the cluster - keeps track of active consumers by receiving heartbeats
             - trigger rebalancing when heartbets stop)
- Group Leader (typ. 1st consumer joined to group, receives info about cur. consumers from group coordinator
             - allocates partitions to consumers in the group) when goes down - trnasferred to next broker

--- Batching & Polling ---
Poll Interval
FETCH.MIN.BYTES             (Def. 1Byte)
FETCH.MAX.WAIT.MS           (Def. 500ms)
MAX.PARTITION.FETCH.BYTES   (Def. 1MB)

--- Commit messages ---
EnableAutoCommit  - does NOT guarantee reliable messages processing:
        manual commit better: when consumer goes down before message received
        then this message can be lost because kafka receives commit notification and will never been restored

        manual commit : synchronous & asynchronous


AutoCommitIntervalMs (Def. 5000)

############### Best practices notes ###############

--- Partition sizing:

    1 - can not be DEcreased when topic has been created;
    2 - fewer partitions -> fewer brokers to handle topic + serialized processing  - leads to lags on producers & consumers side
      + some of brokers & consumers could be starved/overloaded
    3 - excessive partitions per broker results in idling resources: file handles & memory + number of relications overhead
    - summary:
        - plan partition count before creating topics
        - partition counts >= number of expected consumers in a group
        - in a cluster use as many brokers as possible (incl. replicas)
        - number of unique keys in messages > number of partitions (otherwise some partitions will receive no messages) + equally distributed (hash calc)
        - provide load testing to measure right size of partition (overall peak load, check partition lag)

--- Messages:

    1 - keep the message size as small as possible (do not include blob - better pass filename)
    2 - use proper message format (binary preferred like Avro)
    3 - use schema repository to share message schema
    4 - use keys for (even distribution in partitions; if you need in ordered messages in partition by key)
        w/o key Kafka will use round robin distribution around partitions

--- Consumer settings:

    1 - choose right group size (proper to partition number)
    2 - Choose batching parameters based on use case (right batching can reduce network round trips)
        - for real time data better to use smaller batch size & polling interval - decrease latency
    3 - use manual commits to ensure processing reliability (see Consumer scal. options.)
    4 - use non-blocking processing (async) for higher throughput
    5 - test for failure conditions (e.g. when consumer or broker going down)

--- Resiliency:
    
    1 - use replication to stay with broker failures
    2 - distribute brokers across diff hardware racks (helps when physical node goes down)
    3 - use acknowledgements in producer for guaranteed delivery
    4 - use manual commits on consumer side
    5 - configure multiple controllers (at least 3 recommended)
    6 - use mirroring if geo-redundancy is required
    7 - test resiliency use cases (when nodes go down & recovered)










